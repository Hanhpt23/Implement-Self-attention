{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPv8+qunaQLhZ6+FglauWUB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanhpt23/Implement-Self-attention/blob/main/Self_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This project aims to introduce self-attention.\n",
        "The self-attention process involves the following steps:\n",
        "\n",
        "1. The input for self-attention is a sequence. From this input sequence, we multiply it by three weight matrices: query (Q), key (K), and value (V) to get Q, K and V matrices.\n",
        "\n",
        "2. We calculate the attention weights by multiplying Q with the transpose of K and then applying a softmax function to the resulting values.\n",
        "\n",
        "3. The attention is calculated by multiplying the output of the softmax function with the value (V)."
      ],
      "metadata": {
        "id": "GJb__hdDRpzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "7sAmIBkrRdz_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4l2xLd1KU5Xx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxGSnP9hQ8OJ",
        "outputId": "7d839be5-c744-44cc-c215-b50c700cda63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3136, 768])\n"
          ]
        }
      ],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.1):\n",
        "        '''dim is the length of the input sequences'''\n",
        "\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.kv = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "        B, N, C = x.shape           # [Batchsize (B) x num_patch (N) x embed_size (C)]\n",
        "\n",
        "        # Q matrix [B x N x C] ----> [B x N x h x (C/h)] ----> [B x h x N x S]; S = C/h\n",
        "        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "        # We use a reduction technique to reduce the computational complex of\n",
        "        # [B x N x C] ----> [B x N/2 x 2 x h x S] ----> [2 x B x h x N/2 x S]\n",
        "        kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        k, v = kv[0], kv[1] # [B x h x N/2 x S], [B x h x N/2 x S]\n",
        "\n",
        "        # Calculate attention weight [B x h x N x S] x [B x h x S x N/2] = [B x h x N x N/2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # Calculate attention [B x h x N x N/2] x [B x h x N/2 x S] = [B x h x N x S]\n",
        "        # [B x h x N x S] ----> [B x N x h x S] ----> [B x N x (hxS)] = [B x N x C]\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "ax = torch.rand(1, 3136, 768)\n",
        "att = Attention(768)(ax, 56,56)\n",
        "\n",
        "print(att.shape) # torch.Size([1, 3136, 768])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h = 4\n",
        "x = torch.rand((1, 10, 64))\n",
        "B, N, C = x.shape\n",
        "q = x.reshape(B, N, h, C//h).permute(0, 2, 1, 3)\n",
        "print('Query ', q.shape)\n",
        "\n",
        "kv =x.reshape(B, -1, 2,  h, C//h).permute(2, 0, 3, 1, 4)\n",
        "print(kv.shape)\n",
        "\n",
        "k, v = kv[0], kv[1]\n",
        "print('Key ', k.shape, 'Value', v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3KyHrG4Xd9i",
        "outputId": "4a6a1055-8979-4016-a307-40e24b1102c7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query  torch.Size([1, 4, 10, 16])\n",
            "torch.Size([2, 1, 4, 5, 16])\n",
            "Key  torch.Size([1, 4, 5, 16]) Value torch.Size([1, 4, 5, 16])\n"
          ]
        }
      ]
    }
  ]
}